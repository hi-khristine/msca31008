{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain logistic regression isn't looking promising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# read functions.\n",
    "import os\n",
    "for f in os.listdir('fun/'): exec(open('fun/'+f).read())\n",
    "del f\n",
    "\n",
    "# Load data\n",
    "load( 'out/d3-fight-level-standardize-normalize.pkl' )\n",
    "\n",
    "# Change winner to binary 1/0:\n",
    "y[ y == -1 ] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just throw everything in, getting lots of bad p-values (and a pretty bad r-squared)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.622251\n",
      "         Iterations 7\n",
      "                            Results: Logit\n",
      "======================================================================\n",
      "Model:                 Logit             Pseudo R-squared:  0.054     \n",
      "Dependent Variable:    Winner            AIC:               3037.7799 \n",
      "Date:                  2019-11-17 20:53  BIC:               3491.7718 \n",
      "No. Observations:      2314              Log-Likelihood:    -1439.9   \n",
      "Df Model:              78                LL-Null:           -1522.0   \n",
      "Df Residuals:          2235              LLR p-value:       4.1760e-08\n",
      "Converged:             1.0000            Scale:             1.0000    \n",
      "No. Iterations:        7.0000                                         \n",
      "----------------------------------------------------------------------\n",
      "     Coef.      Std.Err.      z    P>|z|      [0.025         0.975]   \n",
      "----------------------------------------------------------------------\n",
      "x1    0.9762        0.6257  1.5602 0.1187        -0.2502        2.2026\n",
      "x2   -0.4957        0.5404 -0.9172 0.3590        -1.5549        0.5636\n",
      "x3   -0.0146        0.4017 -0.0362 0.9711        -0.8020        0.7728\n",
      "x4   -1.1644        0.5165 -2.2545 0.0242        -2.1766       -0.1521\n",
      "x5    0.9630        0.5246  1.8358 0.0664        -0.0652        1.9912\n",
      "x6    1.0404        0.4113  2.5294 0.0114         0.2342        1.8465\n",
      "x7   -0.0150        0.4514 -0.0332 0.9735        -0.8998        0.8698\n",
      "x8    1.2236        0.5503  2.2234 0.0262         0.1450        2.3022\n",
      "x9    0.0497        0.4438  0.1119 0.9109        -0.8201        0.9194\n",
      "x10   0.3648        0.4734  0.7705 0.4410        -0.5631        1.2926\n",
      "x11  -0.5694        0.4743 -1.2004 0.2300        -1.4990        0.3603\n",
      "x12   0.3084        0.8917  0.3458 0.7295        -1.4393        2.0560\n",
      "x13  -0.3764        0.3983 -0.9450 0.3447        -1.1571        0.4043\n",
      "x14   0.0765        1.0691  0.0715 0.9430        -2.0190        2.1719\n",
      "x15  -0.1098        0.4402 -0.2494 0.8030        -0.9726        0.7530\n",
      "x16   0.6631        1.7272  0.3839 0.7010        -2.7222        4.0485\n",
      "x17   0.0604        0.4029  0.1499 0.8808        -0.7293        0.8502\n",
      "x18  -1.5920        1.0208 -1.5596 0.1189        -3.5927        0.4087\n",
      "x19  -0.1143        0.4561 -0.2506 0.8022        -1.0083        0.7797\n",
      "x20   0.5000        0.4955  1.0091 0.3129        -0.4712        1.4712\n",
      "x21   0.0136        0.6680  0.0203 0.9838        -1.2958        1.3229\n",
      "x22  -1.4170        1.1336 -1.2500 0.2113        -3.6389        0.8048\n",
      "x23  -0.1036        2.0797 -0.0498 0.9603        -4.1798        3.9725\n",
      "x24   1.3994        0.6257  2.2367 0.0253         0.1731        2.6257\n",
      "x25  -0.2617        0.5728 -0.4569 0.6477        -1.3844        0.8610\n",
      "x26   0.1593        0.9093  0.1752 0.8609        -1.6228        1.9414\n",
      "x27   0.6963        2.7727  0.2511 0.8017        -4.7380        6.1307\n",
      "x28   6.0617        6.1155  0.9912 0.3216        -5.9246       18.0479\n",
      "x29   6.4284        6.5429  0.9825 0.3259        -6.3955       19.2523\n",
      "x30   5.8449        5.8386  1.0011 0.3168        -5.5985       17.2883\n",
      "x31   0.5770        1.2846  0.4492 0.6533        -1.9407        3.0947\n",
      "x32 -10.2884       11.8282 -0.8698 0.3844       -33.4712       12.8945\n",
      "x33  -0.8044        0.3997 -2.0127 0.0442        -1.5878       -0.0211\n",
      "x34  -0.8910        0.4648 -1.9171 0.0552        -1.8019        0.0199\n",
      "x35  -0.1789        0.4535 -0.3946 0.6932        -1.0678        0.7100\n",
      "x36  -0.2382        0.6000 -0.3969 0.6914        -1.4142        0.9379\n",
      "x37   0.9039        0.5156  1.7531 0.0796        -0.1067        1.9145\n",
      "x38   0.8784        0.6963  1.2615 0.2071        -0.4863        2.2432\n",
      "x39   0.9659        0.5028  1.9212 0.0547        -0.0195        1.9514\n",
      "x40   1.5399        1.5031  1.0245 0.3056        -1.4062        4.4859\n",
      "x41  -0.5236        0.5245 -0.9982 0.3182        -1.5517        0.5045\n",
      "x42  -0.8065        1.8031 -0.4473 0.6547        -4.3406        2.7275\n",
      "x43  -0.3303        0.4626 -0.7141 0.4752        -1.2369        0.5763\n",
      "x44   0.9209        0.9664  0.9529 0.3406        -0.9732        2.8150\n",
      "x45  -3.5966        0.4559 -7.8889 0.0000        -4.4901       -2.7030\n",
      "x46  -0.8677        0.3838 -2.2610 0.0238        -1.6198       -0.1155\n",
      "x47  -0.2048        0.3707 -0.5524 0.5807        -0.9314        0.5218\n",
      "x48  -0.7876        0.4292 -1.8352 0.0665        -1.6287        0.0535\n",
      "x49   0.7967        0.3996  1.9939 0.0462         0.0135        1.5798\n",
      "x50  -0.2202 18904586.1290 -0.0000 1.0000 -37052308.1756 37052307.7352\n",
      "x51  -0.0750 25036338.8159 -0.0000 1.0000 -49070322.4589 49070322.3088\n",
      "x52   0.1325 18988024.1781  0.0000 1.0000 -37215843.3942 37215843.6592\n",
      "x53   0.8277 11576970.1881  0.0000 1.0000 -22690443.7911 22690445.4465\n",
      "x54  -1.0214 12822445.8466 -0.0000 1.0000 -25131533.0746 25131531.0317\n",
      "x55  -0.7593 16915845.2475 -0.0000 1.0000 -33154448.2124 33154446.6937\n",
      "x56   0.2161 16866814.2431  0.0000 1.0000 -33058348.2343 33058348.6665\n",
      "x57   0.1812 31987621.6106  0.0000 1.0000 -62694586.1268 62694586.4891\n",
      "x58   0.7833 11180205.8577  0.0000 1.0000 -21912800.0375 21912801.6041\n",
      "x59   0.1089 15519027.9702  0.0000 1.0000 -30416735.7879 30416736.0056\n",
      "x60  -0.3960           nan     nan    nan            nan           nan\n",
      "x61   0.4251           nan     nan    nan            nan           nan\n",
      "x62  -0.2797           nan     nan    nan            nan           nan\n",
      "x63   0.7359           nan     nan    nan            nan           nan\n",
      "x64   0.2701           nan     nan    nan            nan           nan\n",
      "x65  -0.2116           nan     nan    nan            nan           nan\n",
      "x66  -1.3515           nan     nan    nan            nan           nan\n",
      "x67   0.4245           nan     nan    nan            nan           nan\n",
      "x68   0.7450           nan     nan    nan            nan           nan\n",
      "x69  -0.6658           nan     nan    nan            nan           nan\n",
      "x70   0.8657           nan     nan    nan            nan           nan\n",
      "x71   0.0591           nan     nan    nan            nan           nan\n",
      "x72   0.5473           nan     nan    nan            nan           nan\n",
      "x73   0.6947           nan     nan    nan            nan           nan\n",
      "x74   1.1461           nan     nan    nan            nan           nan\n",
      "x75   1.0957           nan     nan    nan            nan           nan\n",
      "x76   0.7871           nan     nan    nan            nan           nan\n",
      "x77   0.6234           nan     nan    nan            nan           nan\n",
      "x78   0.6398           nan     nan    nan            nan           nan\n",
      "x79   0.1747           nan     nan    nan            nan           nan\n",
      "x80 -50.1977           nan     nan    nan            nan           nan\n",
      "x81  -0.1430           nan     nan    nan            nan           nan\n",
      "x82   0.7140           nan     nan    nan            nan           nan\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bryce\\documents\\python37\\lib\\site-packages\\statsmodels\\base\\model.py:1286: RuntimeWarning: invalid value encountered in sqrt\n",
      "  bse_ = np.sqrt(np.diag(self.cov_params()))\n",
      "c:\\users\\bryce\\documents\\python37\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "c:\\users\\bryce\\documents\\python37\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "c:\\users\\bryce\\documents\\python37\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    }
   ],
   "source": [
    "# Naive throw everything in\n",
    "logit_model = sm.Logit( y, X )\n",
    "result = logit_model.fit()\n",
    "print(result.summary2())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean wins: 0.632238547968885 \n",
      "Mean predict: 0.5479612642105314\n",
      "\n",
      "Accuracy predicting all wins:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       851\n",
      "           1       0.63      1.00      0.77      1463\n",
      "\n",
      "    accuracy                           0.63      2314\n",
      "   macro avg       0.32      0.50      0.39      2314\n",
      "weighted avg       0.40      0.63      0.49      2314\n",
      "\n",
      "Accuracy with varying cutoffs:\n",
      "\n",
      "0.0: \t f1-score: 0.39   \t precision 0.63   \t recall: 1.0\n",
      "0.1: \t f1-score: 0.39   \t precision 0.63   \t recall: 1.0\n",
      "0.2: \t f1-score: 0.43   \t precision 0.64   \t recall: 0.99\n",
      "0.3: \t f1-score: 0.53   \t precision 0.66   \t recall: 0.95\n",
      "0.4: \t f1-score: 0.61   \t precision 0.7   \t recall: 0.86\n",
      "0.5: \t f1-score: 0.65   \t precision 0.75   \t recall: 0.71\n",
      "0.6: \t f1-score: 0.63   \t precision 0.82   \t recall: 0.54\n",
      "0.7: \t f1-score: 0.51   \t precision 0.85   \t recall: 0.29\n",
      "0.8: \t f1-score: 0.38   \t precision 0.91   \t recall: 0.12\n",
      "0.9: \t f1-score: 0.28   \t precision 1.0   \t recall: 0.01\n",
      "1.0: \t f1-score: 0.27   \t precision 0.0   \t recall: 0.0\n",
      "\n",
      "Seems like a cutoff of around .5 gives us way above average wins \n",
      "while participating in a large number of fights.\n",
      "We are capturing 71% of the wins (recall) and winning 75% of the time (precision).\n",
      "Strangely though, we could win 63% of the time and capture 100% of the wins by\n",
      "always betting to win.\n",
      "I guess we need to think about betting and what make the most sense.\n",
      "Here are the stats for a .5 cutoff:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.60      0.57       851\n",
      "           1       0.75      0.71      0.73      1463\n",
      "\n",
      "    accuracy                           0.67      2314\n",
      "   macro avg       0.65      0.65      0.65      2314\n",
      "weighted avg       0.68      0.67      0.67      2314\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bryce\\documents\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\bryce\\documents\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\bryce\\documents\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# make predictions and check recall, precision, f1 score.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "pred = result.predict()\n",
    "print( \n",
    "    'Mean wins: %s \\nMean predict: %s\\n' % ( \n",
    "    y.mean(),\n",
    "    pred.mean()\n",
    "))\n",
    "\n",
    "# what is our base level if we predict the majority?\n",
    "print( 'Accuracy predicting all wins:\\n')\n",
    "print( classification_report( \n",
    "    y, \n",
    "    [ 1 for x in pred ]\n",
    "))\n",
    "\n",
    "# what is the outcome of different cutoffs?\n",
    "print( 'Accuracy with varying cutoffs:\\n' )\n",
    "for i in range(11): \n",
    "    \n",
    "    icutoff = i/10\n",
    "    \n",
    "    predwin = [ 1 if x > i/10 else 0 for x in pred ]\n",
    "    predloss = [ 0 if x > i/10 else 1 for x in pred ]\n",
    "    \n",
    "    fscorewin = f1_score( y, predwin )\n",
    "    fscoreloss = f1_score( ( y == 0 ) * 1, predloss )    \n",
    "    prec = precision_score( y, predwin )\n",
    "    recall = recall_score( y, predwin )\n",
    "    \n",
    "    print(\n",
    "        '%s: \\t f1-score: %s   \\t precision %s   \\t recall: %s' % ( \n",
    "            i/10, \n",
    "            round( (fscorewin + fscoreloss) / 2, 2 ),\n",
    "            round( prec, 2 ),\n",
    "            round( recall, 2 )\n",
    "    ))\n",
    "    \n",
    "print( '''\n",
    "Seems like a cutoff of around .5 gives us way above average wins \n",
    "while participating in a large number of fights.\n",
    "We are capturing 71% of the wins (recall) and winning 75% of the time (precision).\n",
    "Strangely though, we could win 63% of the time and capture 100% of the wins by\n",
    "always betting to win.\n",
    "I guess we need to think about betting and what make the most sense.\n",
    "Here are the stats for a .5 cutoff:\n",
    "''')\n",
    "\n",
    "print( classification_report( \n",
    "    y, \n",
    "    [ 1 if x > 0.5 else 0 for x in pred ]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove most of the insignificant features to see if something looks better. The only problem is that there isn't much predictive value regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.622251\n",
      "         Iterations 7\n",
      "                            Results: Logit\n",
      "======================================================================\n",
      "Model:                 Logit             Pseudo R-squared:  0.054     \n",
      "Dependent Variable:    Winner            AIC:               3037.7799 \n",
      "Date:                  2019-11-17 20:59  BIC:               3491.7718 \n",
      "No. Observations:      2314              Log-Likelihood:    -1439.9   \n",
      "Df Model:              78                LL-Null:           -1522.0   \n",
      "Df Residuals:          2235              LLR p-value:       4.1760e-08\n",
      "Converged:             1.0000            Scale:             1.0000    \n",
      "No. Iterations:        7.0000                                         \n",
      "----------------------------------------------------------------------\n",
      "     Coef.      Std.Err.      z    P>|z|      [0.025         0.975]   \n",
      "----------------------------------------------------------------------\n",
      "x1    0.9762        0.6257  1.5602 0.1187        -0.2502        2.2026\n",
      "x2   -0.4957        0.5404 -0.9172 0.3590        -1.5549        0.5636\n",
      "x3   -0.0146        0.4017 -0.0362 0.9711        -0.8020        0.7728\n",
      "x4   -1.1644        0.5165 -2.2545 0.0242        -2.1766       -0.1521\n",
      "x5    0.9630        0.5246  1.8358 0.0664        -0.0652        1.9912\n",
      "x6    1.0404        0.4113  2.5294 0.0114         0.2342        1.8465\n",
      "x7   -0.0150        0.4514 -0.0332 0.9735        -0.8998        0.8698\n",
      "x8    1.2236        0.5503  2.2234 0.0262         0.1450        2.3022\n",
      "x9    0.0497        0.4438  0.1119 0.9109        -0.8201        0.9194\n",
      "x10   0.3648        0.4734  0.7705 0.4410        -0.5631        1.2926\n",
      "x11  -0.5694        0.4743 -1.2004 0.2300        -1.4990        0.3603\n",
      "x12   0.3084        0.8917  0.3458 0.7295        -1.4393        2.0560\n",
      "x13  -0.3764        0.3983 -0.9450 0.3447        -1.1571        0.4043\n",
      "x14   0.0765        1.0691  0.0715 0.9430        -2.0190        2.1719\n",
      "x15  -0.1098        0.4402 -0.2494 0.8030        -0.9726        0.7530\n",
      "x16   0.6631        1.7272  0.3839 0.7010        -2.7222        4.0485\n",
      "x17   0.0604        0.4029  0.1499 0.8808        -0.7293        0.8502\n",
      "x18  -1.5920        1.0208 -1.5596 0.1189        -3.5927        0.4087\n",
      "x19  -0.1143        0.4561 -0.2506 0.8022        -1.0083        0.7797\n",
      "x20   0.5000        0.4955  1.0091 0.3129        -0.4712        1.4712\n",
      "x21   0.0136        0.6680  0.0203 0.9838        -1.2958        1.3229\n",
      "x22  -1.4170        1.1336 -1.2500 0.2113        -3.6389        0.8048\n",
      "x23  -0.1036        2.0797 -0.0498 0.9603        -4.1798        3.9725\n",
      "x24   1.3994        0.6257  2.2367 0.0253         0.1731        2.6257\n",
      "x25  -0.2617        0.5728 -0.4569 0.6477        -1.3844        0.8610\n",
      "x26   0.1593        0.9093  0.1752 0.8609        -1.6228        1.9414\n",
      "x27   0.6963        2.7727  0.2511 0.8017        -4.7380        6.1307\n",
      "x28   6.0617        6.1155  0.9912 0.3216        -5.9246       18.0479\n",
      "x29   6.4284        6.5429  0.9825 0.3259        -6.3955       19.2523\n",
      "x30   5.8449        5.8386  1.0011 0.3168        -5.5985       17.2883\n",
      "x31   0.5770        1.2846  0.4492 0.6533        -1.9407        3.0947\n",
      "x32 -10.2884       11.8282 -0.8698 0.3844       -33.4712       12.8945\n",
      "x33  -0.8044        0.3997 -2.0127 0.0442        -1.5878       -0.0211\n",
      "x34  -0.8910        0.4648 -1.9171 0.0552        -1.8019        0.0199\n",
      "x35  -0.1789        0.4535 -0.3946 0.6932        -1.0678        0.7100\n",
      "x36  -0.2382        0.6000 -0.3969 0.6914        -1.4142        0.9379\n",
      "x37   0.9039        0.5156  1.7531 0.0796        -0.1067        1.9145\n",
      "x38   0.8784        0.6963  1.2615 0.2071        -0.4863        2.2432\n",
      "x39   0.9659        0.5028  1.9212 0.0547        -0.0195        1.9514\n",
      "x40   1.5399        1.5031  1.0245 0.3056        -1.4062        4.4859\n",
      "x41  -0.5236        0.5245 -0.9982 0.3182        -1.5517        0.5045\n",
      "x42  -0.8065        1.8031 -0.4473 0.6547        -4.3406        2.7275\n",
      "x43  -0.3303        0.4626 -0.7141 0.4752        -1.2369        0.5763\n",
      "x44   0.9209        0.9664  0.9529 0.3406        -0.9732        2.8150\n",
      "x45  -3.5966        0.4559 -7.8889 0.0000        -4.4901       -2.7030\n",
      "x46  -0.8677        0.3838 -2.2610 0.0238        -1.6198       -0.1155\n",
      "x47  -0.2048        0.3707 -0.5524 0.5807        -0.9314        0.5218\n",
      "x48  -0.7876        0.4292 -1.8352 0.0665        -1.6287        0.0535\n",
      "x49   0.7967        0.3996  1.9939 0.0462         0.0135        1.5798\n",
      "x50  -0.2202 18904586.1290 -0.0000 1.0000 -37052308.1756 37052307.7352\n",
      "x51  -0.0750 25036338.8159 -0.0000 1.0000 -49070322.4589 49070322.3088\n",
      "x52   0.1325 18988024.1781  0.0000 1.0000 -37215843.3942 37215843.6592\n",
      "x53   0.8277 11576970.1881  0.0000 1.0000 -22690443.7911 22690445.4465\n",
      "x54  -1.0214 12822445.8466 -0.0000 1.0000 -25131533.0746 25131531.0317\n",
      "x55  -0.7593 16915845.2475 -0.0000 1.0000 -33154448.2124 33154446.6937\n",
      "x56   0.2161 16866814.2431  0.0000 1.0000 -33058348.2343 33058348.6665\n",
      "x57   0.1812 31987621.6106  0.0000 1.0000 -62694586.1268 62694586.4891\n",
      "x58   0.7833 11180205.8577  0.0000 1.0000 -21912800.0375 21912801.6041\n",
      "x59   0.1089 15519027.9702  0.0000 1.0000 -30416735.7879 30416736.0056\n",
      "x60  -0.3960           nan     nan    nan            nan           nan\n",
      "x61   0.4251           nan     nan    nan            nan           nan\n",
      "x62  -0.2797           nan     nan    nan            nan           nan\n",
      "x63   0.7359           nan     nan    nan            nan           nan\n",
      "x64   0.2701           nan     nan    nan            nan           nan\n",
      "x65  -0.2116           nan     nan    nan            nan           nan\n",
      "x66  -1.3515           nan     nan    nan            nan           nan\n",
      "x67   0.4245           nan     nan    nan            nan           nan\n",
      "x68   0.7450           nan     nan    nan            nan           nan\n",
      "x69  -0.6658           nan     nan    nan            nan           nan\n",
      "x70   0.8657           nan     nan    nan            nan           nan\n",
      "x71   0.0591           nan     nan    nan            nan           nan\n",
      "x72   0.5473           nan     nan    nan            nan           nan\n",
      "x73   0.6947           nan     nan    nan            nan           nan\n",
      "x74   1.1461           nan     nan    nan            nan           nan\n",
      "x75   1.0957           nan     nan    nan            nan           nan\n",
      "x76   0.7871           nan     nan    nan            nan           nan\n",
      "x77   0.6234           nan     nan    nan            nan           nan\n",
      "x78   0.6398           nan     nan    nan            nan           nan\n",
      "x79   0.1747           nan     nan    nan            nan           nan\n",
      "x80 -50.1977           nan     nan    nan            nan           nan\n",
      "x81  -0.1430           nan     nan    nan            nan           nan\n",
      "x82   0.7140           nan     nan    nan            nan           nan\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prior model with incomplete dataset:\n",
    "\n",
    "data = pd.read_csv(\"./out/d_fight_level_dataset_1line.csv\", index_col = 0)\n",
    "\n",
    "# Change winner to binary 1/0:\\n\",\n",
    "data.Winner = data.Winner.apply(lambda x: np.where(x == -1, 0, 1))\n",
    "\n",
    "# Initial features and target\\n\",\n",
    "features = pd.Series(data.columns, index = data.columns)\n",
    "target = \"Winner\"\n",
    "\n",
    "# Remove referree, date, location, winner, title_bout, weight_class, no_of_rounds\\n\",\n",
    "features.drop(index = [\"Referee\", \"date\", \"location\", \"Winner\", \"title_bout\",\n",
    "                       \"weight_class\", \"no_of_rounds\"], inplace = True)\n",
    "\n",
    "# Diff_draw is mostly NA/0\\n\",\n",
    "features.drop(index = \"Diff_draw\", inplace = True)\n",
    "\n",
    "# Lots of win columns\\n\",\n",
    "features.drop(index = [\"Diff_win_by_Decision_Majority\",\n",
    "                       \"Diff_win_by_Decision_Split\",\n",
    "                       \"Diff_win_by_Decision_Unanimous\",\n",
    "                       \"Diff_win_by_KO/TKO\",\n",
    "                       \"Diff_win_by_Submission\",\n",
    "                       \"Diff_win_by_TKO_Doctor_Stoppage\"], inplace = True)\n",
    "\n",
    "# Naive throw everything in\n",
    "logit_model = sm.Logit(data[target], data[features_adj])\n",
    "result = logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['x2' 'x3' 'x7' 'x9' 'x10' 'x11' 'x12' 'x13' 'x14' 'x15' 'x16' 'x17' 'x19'\\n 'x20' 'x21' 'x22' 'x23' 'x25' 'x26' 'x27' 'x28' 'x29' 'x30' 'x31' 'x32'\\n 'x35' 'x36' 'x38' 'x40' 'x41' 'x42' 'x43' 'x44' 'x47' 'x50' 'x51' 'x52'\\n 'x53' 'x54' 'x55' 'x56' 'x57' 'x58' 'x59'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2319919ecf44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures_adj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"P>|z|\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m.15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlogit_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures_adj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bryce\\documents\\python37\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4321\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4322\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4323\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4324\u001b[0m         )\n\u001b[0;32m   4325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bryce\\documents\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3912\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3913\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3914\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3916\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bryce\\documents\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3944\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3945\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3946\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3947\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bryce\\documents\\python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5340\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not found in axis\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5341\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['x2' 'x3' 'x7' 'x9' 'x10' 'x11' 'x12' 'x13' 'x14' 'x15' 'x16' 'x17' 'x19'\\n 'x20' 'x21' 'x22' 'x23' 'x25' 'x26' 'x27' 'x28' 'x29' 'x30' 'x31' 'x32'\\n 'x35' 'x36' 'x38' 'x40' 'x41' 'x42' 'x43' 'x44' 'x47' 'x50' 'x51' 'x52'\\n 'x53' 'x54' 'x55' 'x56' 'x57' 'x58' 'x59'] not found in axis\""
     ]
    }
   ],
   "source": [
    "features_adj = features.drop(index = result.summary2().tables[1].index[result.summary2().tables[1][\"P>|z|\"] > .15])\n",
    "logit_model = sm.Logit(data[target], data[features_adj])\n",
    "result = logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the effects are different by weight class? These results show more promise in some cases (though probably not enough effectiveness for a betting strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.DataFrame(data.weight_class.value_counts())\n",
    "classes.drop(index = classes.index[np.where(classes.weight_class < 100)], inplace = True)\n",
    "\n",
    "for x in range(len(classes.index)):\n",
    "    df = data.loc[data.weight_class == classes.index[x]]\n",
    "    print(\"Class: \" + classes.index[x])\n",
    "    logit_model = sm.Logit(df[target], df[features_adj])\n",
    "    result = logit_model.fit()\n",
    "    print(result.summary2())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
